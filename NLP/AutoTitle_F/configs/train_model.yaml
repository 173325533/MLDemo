seed: 1314
gpus: [0]
checkpoint_restore: './data/logs/2018-12-06-01-53/95000.checkpoint.pt'
log: './data/logs/'
vocab_path: './data/preprocessed/dataset_50k_600/vocab_50000'
train_ds_path: './data/preprocessed/dataset_50k_600/train.data'
val_ds_path: './data/preprocessed/dataset_50k_600/val.data'
test_ds_path: './data/preprocessed/dataset_50k_600/test.data'
batch_size: 32
shuffle: False
num_workers: 2
pointer_gen: True
is_coverage: False
is_semantic_similary: False
model: 'seq2seq'
optim: 'AdagradCustom'
score_fn: ''
epoch: 35
use_cuda: True
trunc_norm_init_std: 0.0001
rand_unif_init_mag: 0.02
use_maxpool_init_ctx: False
eps: 0.000000000001
cov_loss_wt: 1.0
simil_wt: 0.0001
learning_rate: 0.15
adagrad_init_acc: 0.1
max_grad_norm: 2.0
#learning_rate_decay: 0.95
#start_decay_at: 5
schedule: False
share_vocab: True
emb_dim: 300
hidden_dim: 256
encoder_num_layers: 2
dropout: 0.5
encoder_bidirec: True
decoder_num_layers: 1
decoder_hidden_size: 512
global_emb: False
max_tgt_len: 20
metrics: ['bleu_1', 'bleu_2', 'rouge_l', 'cider']
eval_interval: 1000
save_interval: 1500
print_interval: 10
beam_size: 5
max_dec_steps: 20
min_dec_steps: 5

